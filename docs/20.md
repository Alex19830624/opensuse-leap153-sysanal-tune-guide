# 14 调优内存管理子系统

> 原文:[https://doc . opensuse . org/documentation/leap/tuning/html/book-tuning/cha-tuning-memory . html](https://doc.opensuse.org/documentation/leap/tuning/html/book-tuning/cha-tuning-memory.html)

Applies to openSUSE Leap 15.3

[14.1 Memory usage](cha-tuning-memory.html#cha-tuning-memory-usage)

[14.2 Reducing memory usage](cha-tuning-memory.html#cha-tuning-memory-optimize)

[14.3 Virtual memory manager (VM) tunable parameters](cha-tuning-memory.html#cha-tuning-memory-vm)

[14.4 Monitoring VM behavior](cha-tuning-memory.html#cha-tuning-memory-monitoring)

要理解和调优内核的内存管理行为，首先要了解它是如何工作的，以及如何与其他子系统协作，这一点很重要。

内存管理子系统，也称为虚拟内存管理器，随后将被称为 VM。VM 的作用是为整个内核和用户程序管理物理内存(RAM)的分配。它还负责为用户进程提供虚拟内存环境(通过带有 Linux 扩展的 POSIX APIs 进行管理)。最后，当内存不足时，虚拟机负责释放内存，要么通过调整缓存，要么换出匿名内存。

在检查和调优虚拟机时，需要理解的最重要的事情是如何管理其缓存。虚拟机缓存的基本目标是将交换和文件系统操作(包括网络文件系统)产生的 I/O 成本降至最低。这是通过完全避免 I/O，或者以更好的模式提交 I/O 来实现的。

这些缓存将根据需要使用和填充空闲内存。可用于缓存和匿名内存的内存越多，缓存和交换的效率就越高。但是，如果遇到内存不足的情况，将会调整缓存或换出内存。

对于特定的工作负载，要提高性能，首先要做的是增加内存，并减少必须调整或交换内存的频率。第二件事是通过改变内核参数来改变缓存的管理方式。

最后，还应该检查和调优工作负载本身。如果允许应用程序运行更多的进程或线程，并且每个进程都在文件系统中自己的区域运行，那么虚拟机缓存的效率就会降低。内存开销也会增加。如果应用程序分配自己的缓冲区或缓存，更大的缓存将意味着更少的内存可用于虚拟机缓存。然而，更多的进程和线程可能意味着更多的重叠和管道 I/O 机会，并可能更好地利用多核。为了获得最佳结果，需要进行实验。

## 14.1 内存使用量

内存分配一般可以分为【钉住】(也称为【不可回收】)【可回收】或【可交换】。

### 14.1.1 匿名记忆

匿名内存倾向于程序堆和堆栈内存(例如`>malloc()`)。它是可回收的，除非在特殊情况下，如`mlock`或没有可用的交换空间。匿名内存必须先写入交换区，然后才能被回收。由于分配和访问模式的原因，交换 I/O(换入和换出页面)的效率往往不如 pagecache I/O。

### 14 . 1 . 2【page cache】

文件数据的缓存。当从磁盘或网络中读取文件时，内容存储在 pagecache 中。如果页面缓存中的内容是最新的，则不需要访问磁盘或网络。tmpfs 和共享内存段计入 pagecache。

当写入文件时，新数据在被写回磁盘或网络(使其成为回写缓存)之前存储在 pagecache 中。当一个页面有尚未写回的新数据时，称为【脏】。未分类为脏的页面是干净。如果内存不足，只需释放页面即可回收干净的页面缓存页面。在回收脏页之前，必须先将其清理干净。

### 14.1.3 缓冲缓存

这是一种用于块设备的 pagecache(例如/dev/sda)。文件系统通常在访问其磁盘上元数据结构(如 inode 表、分配位图等)时使用 buffercache。Buffercache 可以像 pagecache 一样回收。

### 14.1.4 缓冲头

缓冲区头是小的辅助结构，倾向于在页面缓存访问时分配。当 pagecache 或 buffercache 页面干净时，通常可以轻松回收它们。

### 14.1.5 写回

随着应用程序写入文件，pagecache 变脏，buffercache 也可能变脏。当脏内存量达到以字节为单位的指定页面数(*VM . dirty _ background _ bytes*)时，或者当脏内存量达到占总内存的特定比例(*VM . dirty _ background _ ratio*)时，或者当页面变脏的时间超过指定的时间量(*VM . dirty _ expire _ centisecs*)时，内核从页面首先变脏的文件开始写回页面。背景字节和比率是互斥的，设置一个会覆盖另一个。刷新线程在后台执行写回，并允许应用程序继续运行。如果 I/O 跟不上弄脏 pagecache 的应用程序，并且脏数据达到临界设置(*VM . dirty _ bytes*或*VM . dirty _ ratio*)，则应用程序开始被节流，以防止脏数据超过此阈值。

### 14.1.6 预读

虚拟机监控文件访问模式，并可能尝试执行预读。Readahead 将文件系统中尚未请求的页面读入 pagecache。这样做是为了允许提交更少、更大的 I/O 请求(效率更高)。并且 I/O 被流水线化(I/O 在应用程序运行的同时执行)。

### 14.1.7 VFS 缓存

#### 14.1.7.1T2【索引节点缓存】T3

这是每个文件系统的索引节点结构的内存缓存。这些属性包括文件大小、权限和所有权，以及指向文件数据的指针。

#### 14.1.7.2 目录条目缓存

这是系统中目录条目的内存缓存。它们包含一个名称(文件的名称)、它所引用的 inode 和子条目。当遍历目录结构和按名称访问文件时，会用到这个缓存。

## 14.2 减少内存使用

### 14.2.1 减少 malloc(匿名)的使用

与旧版本相比，运行在 openSUSE Leap15.3 上的应用可以分配更多内存。这是因为`glibc`在分配用户空间内存时改变了它的默认行为。有关这些参数的解释，请参见[http://www . GNU . org/s/libc/manual/html _ node/Malloc-tuned-parameters . html](http://www.gnu.org/s/libc/manual/html_node/Malloc-Tunable-Parameters.html)。

要恢复与旧版本类似的行为，M _ MMAP _ 阈值应设置为 128*1024。这可以通过从应用程序调用 mallopt()来完成，或者通过在运行应用程序之前设置 MALLOC_MMAP_THRESHOLD 环境变量来完成。

### 14.2.2 减少内核内存开销

可回收的内核内存(缓存，如上所述)将在内存不足时自动调整。大多数其他内核内存不容易减少，而是由内核的工作负载决定的。

降低用户空间工作负载的需求将减少内核内存的使用(更少的进程、更少的打开文件和套接字等)。)

### 14.2.3 内存控制器(memory cgroups)

如果不需要内存 cgroups 特性，可以通过在内核命令行上传递 cgroup_disable=memory 来关闭它，从而稍微减少内核的内存消耗。即使没有配置任何内存组，当内存组可用时，也会有少量的记帐开销，因此性能也会略有提高。

## 14.3 虚拟内存管理器(VM)可调参数

在调优虚拟机时，应该理解某些更改需要时间来影响工作负载并完全生效。如果工作负载在一天中不断变化，那么它在不同时间的表现可能会非常不同。在某些情况下增加吞吐量的更改可能会在其他情况下降低吞吐量。

### 14.3.1 回收比率

`/proc/sys/vm/swappiness`

该控制用于定义内核相对于 pagecache 和其他缓存换出匿名内存的积极程度。增加该值会增加交换的数量。默认值为`60`。

与其他 I/O 相比，交换 I/O 的效率往往要低得多。但是，一些 pagecache 页面的访问频率要比很少使用的匿名内存高得多。这里应该找到正确的平衡。

如果在减速期间观察到交换活动，则可能值得降低该参数。如果存在大量 I/O 活动，并且系统中的 pagecache 数量相当少，或者如果有大型休眠应用程序正在运行，增加该值可能会提高性能。

请注意，换出的数据越多，系统在需要时将数据换回的时间就越长。

`/proc/sys/vm/vfs_cache_pressure`

该变量控制内核回收用于缓存 VFS 缓存的内存的趋势，而不是页面缓存和交换。增加该值会提高 VFS 缓存的回收速度。

除了通过实验，很难知道什么时候应该改变这种情况。`slabtop`命令(包`procps`的一部分)显示了内核使用的顶级内存对象。vfs 缓存是“dentry”和“*_inode_cache”对象。如果相对于 pagecache 来说，它们消耗了大量内存，那么尝试增加压力可能是值得的。也有助于减少交换。默认值为`100`。

`/proc/sys/vm/min_free_kbytes`

这控制了由特殊保留(包括 " 【原子】" " )分配(那些不能等待回收的分配)保持空闲以供使用的内存量。这通常不应该降低，除非针对内存使用对系统进行了非常仔细的调优(通常对嵌入式应用而非服务器应用有用)。如果日志中频繁出现页面分配失败消息和堆栈跟踪，可以增加 min_free_kbytes，直到错误消失。如果这些消息很少出现，就没有必要担心。默认值取决于 RAM 的大小。

`/proc/sys/vm/watermark_scale_factor`

一般来说，空闲内存有高、低和最小水位线。当达到低水位线时，然后`kswapd`唤醒以在后台回收内存。它会保持唤醒状态，直到可用内存达到高水位线。当达到低水位线时，应用程序将停止并回收内存。

`watermark_scale_factor`定义了在 kswapd 被唤醒之前节点/系统中剩余的内存量，以及在 kswapd 返回睡眠状态之前需要释放多少内存。单位是以 10，000 为单位。默认值 10 表示水位线之间的距离是节点/系统中可用内存的 0.1%。最大值是 1000，即 10%的内存。

由`/proc/vmstat`中的`allocstall`计算的在直接回收中频繁停止的工作负载可能会从更改该参数中受益。类似地，如果`kswapd`过早睡眠，如`kswapd_low_wmark_hit_quickly`所述，那么这可能表明为避免停顿而保持空闲的页面数量太低。

### 14.3.2 写回参数

自 openSUSE Leap 10 以来，写回行为的一个重要变化是，对文件支持的 mmap()内存的修改会立即被视为脏内存(并会被写回)。而在以前，它只在被取消映射后、在 msync()系统调用时或在内存压力很大的情况下才会被写回。

一些应用程序不希望 mmap 修改受到这种写回行为的影响，性能可能会降低。增加写回比率和时间可以改善这种类型的放缓。

`/proc/sys/vm/dirty_background_ratio`

这是可用和可回收内存总量的百分比。当脏页缓存的数量超过此百分比时，写回线程开始写回脏内存。默认值为`10` (%)。

`/proc/sys/vm/dirty_background_bytes`

这包含后台内核刷新程序线程将开始写回的脏内存量。`dirty_background_bytes`是`dirty_background_ratio`的对应物。如果其中一个被设置，另一个将自动读作`0`。

`/proc/sys/vm/dirty_ratio`

类似于`dirty_background_ratio`的百分比值。当超过这个值时，想要写入页面缓存的应用程序将被阻塞，并等待内核后台刷新线程来减少脏内存的数量。默认值为`20` (%)。

`/proc/sys/vm/dirty_bytes`

该文件控制与`dirty_ratio`相同的可调参数，但是脏内存的数量是以字节为单位，而不是可回收内存的百分比。由于`dirty_ratio`和`dirty_bytes`控制同一个可调参数，如果其中一个被设置，另一个将自动读取为`0`。`dirty_bytes`允许的最小值是两页(字节)；低于此限制的任何值都将被忽略，旧配置将被保留。

`/proc/sys/vm/dirty_expire_centisecs`

内存中脏时间超过此时间间隔的数据将在下次刷新线程唤醒时被写出。根据文件信息节点的修改时间来衡量到期时间。因此，当超过时间间隔时，来自同一文件的多个脏页都将被写入。

`dirty_background_ratio`和`dirty_ratio`共同决定了 pagecache 写回行为。如果这些值增加，更多脏内存会在系统中保留更长时间。随着系统中允许更多脏内存，通过避免写回 I/O 和提交更优化的 I/O 模式来提高吞吐量的机会增加了。然而，更多的脏内存要么会在需要回收内存时损害延迟，要么会在数据完整性点(【同步点】)需要写回磁盘时损害延迟。

### 14.3.3 预读参数

`/sys/block/*<bdev>*/queue/read_ahead_kb`

如果一个或多个进程顺序读取一个文件，内核会提前读取一些数据，以减少进程等待数据可用的时间。提前读取的实际数据量是根据 I/O 的“顺序”程度动态计算的。该参数设置内核为单个文件预先读取的最大数据量。如果您发现从文件中进行大型顺序读取不够快，您可以尝试增加该值。增加得太多可能会导致预读抖动，用于预读的 pagecache 在可以使用之前被回收，或者由于大量无用的 I/O 而降低速度。默认值为`512` (KB)。

### 14.3.4 透明大页面参数

透明的大页面(THP)提供了一种动态分配大页面的方法，既可以根据进程的需要进行分配，也可以通过`khugepaged`内核线程将分配推迟到以后进行。这种方法不同于使用`hugetlbfs`来手动管理它们的分配和使用。具有连续内存访问模式的工作负载可以极大地受益于 THP。当运行具有连续内存访问模式的合成工作负载时，可以观察到页面错误减少了 1000 倍。

有些情况下，THP 可能不受欢迎。由于内存使用率过高，内存访问模式稀疏的工作负载在使用 THP 时性能会很差。例如，对于每个故障，在故障时间可能使用 2 MB 的内存，而不是 4 KB，最终导致过早的页面回收。

THP 的行为可以通过`transparent_hugepage=`内核参数或 sysfs 进行配置。例如，可以通过添加内核参数`transparent_hugepage=never`，重新构建 grub2 配置并重启来禁用它。验证 THP 是否已禁用:

```sh
# cat /sys/kernel/mm/transparent_hugepage/enabled
always madvise [never]
```

如果禁用，值`never`显示在方括号中，如上例所示。值`always`将总是尝试在故障时间使用 THP，但是如果分配失败，则遵从`khugepaged`。值`madvise`将只为应用程序明确指定的地址空间分配 THP。

`/sys/kernel/mm/transparent_hugepage/defrag`

此参数控制应用程序在分配 THP 时的工作量。值`always`是支持 THP 的 openSUSE 42.1 和早期版本的默认值。如果 THP 不可用，应用程序将尝试整理内存碎片。如果内存碎片化且 THP 不可用，则可能会导致应用程序出现大量停顿。

值`madvise`意味着 THP 分配请求只有在应用程序明确请求时才会进行碎片整理。这是 openSUSE 42.2 和更高版本的默认设置。

`defer`仅在 openSUSE 42.2 及更高版本上可用。如果 THP 不可用，应用程序将在 THP 不可用时退回到使用小页面。它将唤醒`kswapd`和`kcompactd`内核线程在后台整理内存碎片，稍后`khugepaged`将分配一个 THP。

如果 THP 不可用，最后一个选项`never`将使用小页面，但不会发生其他动作。

### 14 . 3 . 5khuge 分页参数

当`transparent_hugepage`设置为`always`或`madvise`时，khugepaged 将自动启动，当设置为`never`时，khuge paged 将自动关闭。通常这在低频率下运行，但是行为是可以调整的。

`/sys/kernel/mm/transparent_hugepage/khugepaged/defrag`

值 0 将禁用`khugepaged`，即使 THP 在故障时仍可使用。这对于延迟敏感的应用来说可能很重要，这些应用受益于 THP，但是如果`khugepaged`试图更新应用内存使用，则不能容忍停顿。

`/sys/kernel/mm/transparent_hugepage/khugepaged/pages_to_scan`

该参数控制`khugepaged`一次扫描多少页。扫描识别出可以作为 THP 重新分配的小页面。增加该值将在后台更快地分配 THP，但会占用 CPU 资源。

`/sys/kernel/mm/transparent_hugepage/khugepaged/scan_sleep_millisecs`

`khugepaged`每次通过后，休眠一段由该参数指定的短时间间隔，以限制 CPU 的使用量。减小该值将在后台更快地分配 THP，但会占用 CPU 资源。值为 0 将强制连续扫描。

`/sys/kernel/mm/transparent_hugepage/khugepaged/alloc_sleep_millisecs`

该参数控制`khugepaged`在后台分配 THP 等待`kswapd`和`kcompactd`采取行动失败的情况下将休眠多长时间。

`khugepaged`的其余参数很少对性能调整有用，但在`/usr/src/linux/Documentation/vm/transhuge.txt`中有完整的记录

### 14.3.6 进一步 VM 参数

有关 VM 可调参数的完整列表，请参见`/usr/src/linux/Documentation/sysctl/vm.txt`(安装`kernel-source`包后可用)。

## 14.4 监控虚拟机行为

一些有助于监控虚拟机行为的简单工具:

1.  vmstat:这个工具很好地概述了 VM 正在做什么。详见[第 2.1.1 节、`vmstat`、](cha-util.html#sec-util-multi-vmstat "2.1.1. vmstat")。

2.  这个文件给出了内存使用的详细分类。详见[第 2.4.2 节“详细内存使用:`/proc/meminfo`”](cha-util.html#sec-util-memory-meminfo "2.4.2. Detailed memory usage: /proc/meminfo")。

3.  `slabtop`:这个工具提供了内核块内存使用的详细信息。buffer_head、dentry、inode_cache、ext3_inode_cache 等。是主要的储藏处。该命令在`procps`包中可用。

4.  这个文件给出了内部虚拟机行为的详细分类。其中包含的信息因实施而异，可能并不总是可用。有些信息在`/proc/meminfo`中是重复的，其他信息可以由实用程序以友好的方式呈现。为了获得最大的效用，需要长时间监控该文件，以观察其变化率。难以从其他来源获得的最重要的信息如下:

    `pgscan_kswapd_*, pgsteal_kswapd_*`

    这些分别报告了自系统启动以来由`kswapd`扫描和回收的页数。这些值之间的比率可以解释为回收效率，低效率意味着系统正在努力回收内存，并且可能会发生系统颠簸。这里的光活动一般不需要关注。

    `pgscan_direct_*, pgsteal_direct_*`

    它们分别报告应用程序直接扫描和回收的页数。这与`allocstall`计数器的增加相关。这比`kswapd`活动更严重，因为这些事件表明进程正在停止。这里的大量活动与`kswapd`和`pgpgin`、`pgpout`的高比率和/或`pswapin`或`pswpout`的高比率结合在一起，是系统正在剧烈颠簸的迹象。

    使用跟踪点可以获得更详细的信息。

    `thp_fault_alloc, thp_fault_fallback`

    这些计数器对应于应用程序直接分配的 THP 数量，以及 THP 不可用和使用小页面的次数。通常，高回退率是无害的，除非应用程序对 TLB 压力非常敏感。

    `thp_collapse_alloc, thp_collapse_alloc_failed`

    这些计数器对应于`khugepaged`分配了多少 THP，以及 THP 不可用和使用小页面的次数。高回退率意味着系统是碎片化的，即使应用程序的内存使用允许，也不会使用 THP。这只是对 TLB 压力敏感的应用的问题。

    `compact_*_scanned, compact_stall, compact_fail, compact_success`

    当 THP 启用且系统碎片化时，这些计数器可能会增加。当应用停止分配 THP 时,`compact_stall`递增。剩余的计数器计算扫描的页数、成功或失败的碎片整理事件数。