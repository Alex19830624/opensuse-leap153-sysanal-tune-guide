# 15 调整网络

> 原文:[https://doc . opensuse . org/documentation/leap/tuning/html/book-tuning/cha-tuning-network . html](https://doc.opensuse.org/documentation/leap/tuning/html/book-tuning/cha-tuning-network.html)

Applies to openSUSE Leap 15.3

[15.1 Configurable kernel socket buffers](cha-tuning-network.html#sec-tuning-network-buffers)

[15.2 Detecting network bottlenecks and analyzing network traffic](cha-tuning-network.html#sec-tuning-network-analyzing)

[15.3 Netfilter](cha-tuning-network.html#sec-tuning-network-netfilter)

[15.4 Improving the network performance with receive packet steering (RPS)](cha-tuning-network.html#sec-tuning-network-rps)

网络子系统很复杂，其调整高度依赖于系统使用场景和外部因素，如网络中的软件客户端或硬件组件(交换机、路由器或网关)。Linux 内核的目标更多的是可靠性和低延迟，而不是低开销和高吞吐量。其他设置可能意味着安全性较低，但性能更好。

## 15.1 可配置内核套接字缓冲区

网络主要基于 TCP/IP 协议和用于通信的套接字接口；有关 TCP/IP 的更多信息，请参见书*参考*，第 13 章“基本网络”。Linux 内核处理它通过套接字缓冲区中的套接字接口接收或发送的数据。这些内核套接字缓冲区是可调的。

![Important](img/462f4e35442e53ad1c499613dbaf1667.png "Important")

###### 重要提示:TCP 自动调整

从内核版本 2.6.17 开始，存在最大缓冲区大小为 4 MB 的完全自动调整。这意味着手动调整通常不会显著提高网络性能。通常最好不要接触下面的变量，或者至少仔细检查调优工作的结果。

如果您从较旧的内核进行更新，建议您删除手动 TCP 调整，以支持自动调整功能。

`/proc`文件系统中的特殊文件可以修改内核套接字缓冲区的大小和行为；关于`/proc`文件系统的一般信息，参见[第 2.6 节，“T2 文件系统”](cha-util.html#sec-util-proc "2.6. The /proc file system")。在以下位置查找网络相关文件:

```
/proc/sys/net/core
/proc/sys/net/ipv4
/proc/sys/net/ipv6
```

通用`net`变量在内核文档(`linux/Documentation/sysctl/net.txt`)中有解释。特殊的`ipv4`变量在`linux/Documentation/networking/ip-sysctl.txt`和`linux/Documentation/networking/ipvs-sysctl.txt`中解释。

例如，在`/proc`文件系统中，可以为所有协议设置最大套接字接收缓冲区和最大套接字发送缓冲区，或者只为 TCP 协议设置这两个选项(在`ipv4`中)，从而覆盖所有协议的设置(在`core`中)。

`/proc/sys/net/ipv4/tcp_moderate_rcvbuf`

如果`/proc/sys/net/ipv4/tcp_moderate_rcvbuf`被设置为`1`，自动调整被激活，缓冲区大小被动态调整。

`/proc/sys/net/ipv4/tcp_rmem`

这三个值设置了每个连接的内存接收缓冲区的最小、初始和最大大小。它们定义了实际的内存使用，而不仅仅是 TCP 窗口大小。

`/proc/sys/net/ipv4/tcp_wmem`

与`tcp_rmem`相同，但针对每个连接的内存发送缓冲区。

`/proc/sys/net/core/rmem_max`

设置以限制应用程序可以请求的最大接收缓冲区大小。

`/proc/sys/net/core/wmem_max`

设置以限制应用程序可以请求的最大发送缓冲区大小。

通过`/proc`可以禁用您不需要的 TCP 功能(所有 TCP 功能默认开启)。例如，检查以下文件:

`/proc/sys/net/ipv4/tcp_timestamps`

RFC1323 中定义了 TCP 时间戳。

`/proc/sys/net/ipv4/tcp_window_scaling`

RFC1323 中也定义了 TCP 窗口缩放。

`/proc/sys/net/ipv4/tcp_sack`

选择确认(SACKS)。

使用`sysctl`来读取或写入`/proc`文件系统的变量。`sysctl`比`cat`(用于读取)和`echo`(用于写入)更好，因为它也从`/etc/sysctl.conf`读取设置，因此，那些设置在重启后仍然可靠。使用`sysctl`你可以轻松读取所有变量及其值；as `root`使用以下命令列出 TCP 相关设置:

```
> sudo sysctl -a | grep tcp
```

![Note](img/ff87f0b59d655d477bfebbc447e7a566.png "Note")

###### 注意:调整网络变量的副作用

调整网络变量会影响其他系统资源，如 CPU 或内存使用。

## 15.2 检测网络瓶颈并分析网络流量

在开始网络调优之前，隔离网络瓶颈和网络流量模式非常重要。有一些工具可以帮助您检测这些瓶颈。

以下工具可以帮助分析您的网络流量:`netstat`、`tcpdump`和`wireshark`。Wireshark 是一款网络流量分析器。

## 15.3 净过滤器

Netfilter 内核模块提供了 Linux 防火墙和伪装功能。这是一个高度可配置的基于规则的框架。如果某个规则与数据包匹配，Netfilter 会接受或拒绝该数据包，或者根据规则(如地址转换)的定义采取特殊措施( " 目标 " )。

Netfilter 可以考虑很多属性。因此，定义的规则越多，数据包处理持续的时间就越长。此外，高级连接跟踪可能相当昂贵，因此会降低整个网络的速度。

当内核队列变满时，所有新数据包都会被丢弃，从而导致现有连接失败。“失效开放”功能允许用户暂时禁用数据包检测，并在网络流量大的情况下保持连接。参考请见[https://home . regit . org/netfilter-en/using-nf queue-and-libnetfilter _ queue/](https://home.regit.org/netfilter-en/using-nfqueue-and-libnetfilter_queue/)。

有关更多信息，请参见 Netfilter 和 iptables 项目的主页，[http://www.netfilter.org](http://www.netfilter.org)

## 15.4 通过接收数据包导向(RPS)提高网络性能

现代网络接口设备可以传输如此多的数据包，以至于主机成为实现最高性能的限制因素。为了跟上进度，系统必须能够在多个 CPU 内核之间分配工作。

一些现代网络接口可以通过在硬件中实现多个传输和多个接收队列来帮助将工作分配给多个 CPU 核心。然而，其他的仅配备了单个队列，并且驱动程序必须处理单个串行化流中的所有输入包。为了解决这个问题，操作系统必须“并行化”流，以便在多个 CPU 之间分配工作。在 openSUSE Leap 上，这是通过接收数据包转向(RPS)完成的。RPS 也可以用在虚拟环境中。

RPS 使用 IP 地址和端口号为每个数据流创建一个唯一的散列。使用这种哈希可以确保相同数据流的数据包被发送到相同的 CPU，这有助于提高性能。

RPS 是按网络设备接收队列和接口配置的。配置文件名与以下方案匹配:

```
/sys/class/net/*<device>*/queues/*<rx-queue>*/rps_cpus
```

*<设备>* 代表网络设备，如`eth0`、`eth1`。 *< rx-queue >* 代表接收队列，如`rx-0`、`rx-1`。

如果网络接口硬件只支持单个接收队列，那么只有`rx-0`会存在。如果它支持多个接收队列，那么每个接收队列将有一个 rx- *N* 目录。

这些配置文件包含一个以逗号分隔的 CPU 位图列表。默认情况下，所有位都设置为`0`。通过此设置，RPS 被禁用，因此处理中断的 CPU 也将处理数据包队列。

要启用 RPS 并启用特定 CPU 来处理接口接收队列的数据包，请将它们在位图中的位置值设置为`1`。例如，要使 CPU 0-3 能够处理 eth0 的第一个接收队列的数据包，请以二进制形式将位位置 0-3 设置为 1:`00001111`。然后这个表示需要被转换成十六进制——在这种情况下产生了`F`。使用以下命令设置该十六进制值:

```
> sudo echo "f" > /sys/class/net/eth0/queues/rx-0/rps_cpus
```

如果您想启用 CPU 8-15:

```
1111 1111 0000 0000 (binary)
15     15    0    0 (decimal)
F       F    0    0 (hex)
```

设置十六进制值`ff00`的命令是:

```
> sudo echo "ff00" > /sys/class/net/eth0/queues/rx-0/rps_cpus
```

在 NUMA 机器上，通过配置 RPS 使用同一个 NUMA 节点上的 CPU 作为接口接收队列的中断，可以获得最佳性能。

在非 NUMA 的机器上，所有的 CPU 都可以使用。如果中断率非常高，排除处理网络接口的 CPU 可以提高性能。从`/proc/interrupts`可以确定网络接口使用的 CPU。例如:

```
> sudo cat /proc/interrupts
            CPU0       CPU1       CPU2       CPU3
...
  51:  113915241          0          0          0      Phys-fasteoi   eth0
...
```

在这种情况下，`CPU 0`是`eth0`的唯一 CPU 处理中断，因为只有`CPU0`包含非零值。

在 x86 和 AMD64/Intel 64 平台上，`irqbalance`可用于在 CPU 间分配硬件中断。更多详情见`man 1 irqbalance`。